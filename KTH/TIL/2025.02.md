# 정리

## latent space란?

- 머신 러닝(특히 딥러닝)에서 데이터를 ‘압축된’ 형태로 표현하는 잠재 공간을 의미
- 원본 데이터(예: 이미지, 음성, 텍스트 등)를 인코더(Encoder)와 같은 모델 구조나 수학적 기법을 통해 더 낮은 차원 혹은 추상적인 형태로 내부적으로 표현하는 공간

### Latent Space가 중요한 이유

1. 차원 축소: 원본 데이터가 수백~수천 차원을 가진 고차원 벡터일 때, 모델이 직접 처리하기에는 복잡도가 매우 큽니다. 따라서 낮은 차원으로 압축해 처리하면 계산량이 줄어들고, 중요한 패턴을 더 잘 포착할 수
   있습니다.
2. 특징 추출: 잠재 공간에선 원본 데이터의 핵심적인 특징(예: 이미지의 형태, 색상, 패턴 등)이 내재화되어 있습니다. 이를 통해 데이터 분석을 더 효율적으로 수행할 수 있습니다.
3. 생성 모델: 생성적 적대 신경망(GAN)이나 변분 오토인코더(VAE) 같은 모델은 잠재 공간을 샘플링하거나 조작하여 새로운 데이터를 생성해냅니다.
4. 데이터 구조 해석: 고차원 데이터가 어떻게 분포되어 있는지, 서로 어떤 관계를 가지는지를 잠재 공간 상에서 시각화하거나 이해할 수 있습니다.

## Autoencoder란?

![autoencoder.png](../assets/autoencoder.png)
(https://minimi22.tistory.com/26)

![참고](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4vE-aEeu9kGIHn7aqI0wPg.png)
(https://medium.com/@hugmanskj/autoencoder-%EC%99%80-variational-autoencoder%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81%EC%9D%B8-%EC%9D%B4%ED%95%B4-171b3968f20b)


Autoencoder는 비지도 학습(unsupervised learning) 방법 중 하나로, 입력 데이터를 압축(Encoding)하고 다시 복원(Decoding)하는 신경망.
데이터를 저차원의 latent space(잠재 공간)에 효율적으로 표현하는 것이 핵심 목표

1. Encoder(인코더)

입력 데이터를 압축하여 잠재 공간(latent space)에 표현되는 잠재 벡터(Feature Representation)로 변환합니다.
일반적으로 신경망의 층을 거치며 차원이 점점 줄어듭니다.

2. Latent Space(잠재 공간)

데이터의 Feature 추출되는 부분입니다.
인코더가 입력 데이터의 중요한 요소만을 저장하는 embedding(저차원 표현)을 만듭니다.

3. Decoder(디코더)

인코더에서 생성된 잠재 벡터를 다시 원본 데이터와 유사한 형태로 Decoding(복원)합니다.
일반적으로 인코더 구조를 반대로 뒤집은 형태를 가집니다.

### Autoencoder의 활용 사례
- 차원 축소(Dimensionality Reduction)
  - Autoencoder는 PCA보다 더 강력한 비선형 차원 축소를 수행할 수 있습니다.
- 노이즈 제거(Denoising)
  - Denoising Autoencoder(DAE)는 손상된 이미지나 데이터를 원래 상태로 복원하는 데 유용합니다.
  - Autoencoder는 원본 데이터를 잠재 공간으로 변환한 후 다시 복원하는 과정에서, 중요하지 않은 정보(즉, 노이즈)를 자연스럽게 제거합니다.
    - 특히 **Denoising Autoencoder(DAE)**는 일부러 노이즈를 추가한 데이터를 입력하고, 원래의 깨끗한 데이터를 복원하도록 학습됩니다.
- 이상 탐지(Anomaly Detection)
  - 정상 데이터로 Autoencoder를 훈련한 후, 이상 데이터는 복원 오류가 크게 나타나도록 학습하여 이상 탐지를 수행할 수 있습니다.
  - 따라서 이상 데이터를 입력하면 정상 데이터와 달라 **복원 오류(Reconstruction Error)**가 커지게 됩니다.
- 데이터 생성(Generative Models)
  - Variational Autoencoder(VAE)는 새로운 데이터를 생성하는 데 사용됩니다. GAN(생성적 적대 신경망)과 함께 많이 활용됩니다.
- 추천 시스템(Recommendation Systems)
  - 사용자 행동 데이터를 인코딩하여 추천 시스템의 효율을 높일 수 있습니다.


## VAE(Variational Autoencoder)

- 오토인코더(Autoencoder)의 한계  
  - 잠재공간(latent space)의 해석이 어렵다.  
  - 학습된 잠재공간이 연속적이지 않다.  
  - 생성된 데이터의 품질이 낮거나 원본과 차이가 크다.  

- Variational Autoencoder(VAE)의 역할  
  - 오토인코더의 한계를 보완하기 위해 등장.  
  - 잠재공간을 해석 가능하고, 연속적으로 만들며, 더 나은 데이터 생성을 가능하게 함.  

> VAE를 사용하면 오토인코더보다 더 의미 있는 잠재공간을 학습하고, 고품질의 데이터를 생성할 수 있음.

### VAE가 오토인코더의 한계를 극복 원리

1. 잠재공간을 확률적인 형태로 변환

기존 오토인코더는 데이터를 압축할 때 하나의 고정된 값으로 변환함.
VAE는 각 데이터 포인트를 **하나의 값이 아닌 범위(확률분포)**로 변환함.
즉, 특정 위치가 아니라 여러 가능성을 가진 공간에서 표현됨.
덕분에 잠재공간이 더 연속적이고 의미 있는 구조가 됨.

2. 샘플링 과정에서 부드러운 변화 유도

오토인코더는 학습한 데이터만 복원할 수 있어 새로운 데이터를 생성하기 어려움.
VAE는 확률적 샘플링을 통해 새로운 데이터를 만들어낼 수 있도록 유도함.
예를 들어, 손글씨 숫자 데이터를 학습한 후 새로운 숫자를 자연스럽게 생성 가능.
비슷한 숫자끼리는 가까운 위치에 배치되어 있어 연속적인 변화가 가능함.

3. 학습된 공간을 일정한 구조로 유지

기존 오토인코더는 학습된 공간이 무작위로 배치되기 쉬움.
VAE는 잠재공간이 특정한 구조를 유지하도록 추가적인 규칙을 적용함.
예를 들어, 특정 패턴이 있는 데이터는 잠재공간에서도 비슷한 위치에 정렬됨.
덕분에 새로운 데이터를 생성할 때도 일관된 결과가 나옴.

#### 결론

VAE는 잠재공간을 확률적인 방식으로 변환하고, 샘플링을 통해 자연스럽게 데이터를 생성할 수 있도록 학습하기 때문에

- 잠재공간 해석이 쉬워지고,
- 연속적인 데이터 변환이 가능하며,
- 고품질의 새로운 데이터를 생성할 수 있음.

#### 참고이미지

![참고](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nGFy96r63GwSE_EsJDLMDw.png)
![VAE](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r1R0cxCnErWgE0P4Q-hI0Q.jpeg)
(https://medium.com/geekculture/variational-autoencoder-vae-9b8ce5475f68)