### 2025-03-04

### TIL

<br>

#### GANs (적대적 생성 신경망): Generator(생성자), Discriminator(판별자) 두가지 모델이 서로 속이고 판별하면서 경쟁을 해 학습하는 신경망.

#### Generator (생성 모델): 임의의 노이즈를 입력으로 받아 생성된 데이터를 출력. 판별자를 진짜 데이터인 것처럼 속여야한다.

#### Discriminator (판별 모델): 생성된 데이터를 입력으로 받아 실제 데이터(Real)인지 생성된 데이터(fake)인지 출력. 생성자가 만든 데이터의 진위를 파악해야한다.

#### GANs 목적함수: 둘은 적대적으로 학습됨 (min & max)
- Two-Player Zero-Sum Game
- Discriminator는 최대화
- Generator는 최소화
- GANs의 목적 함수는 생성 데이터 분포와 실제 데이터 분포가 동일한 분포에서 최적된다.



- GANs 목적 함수는 실제로는 잘 동작하지 않는다. 그래서 반대방향으로 개선되게 된다.

#### 다양한 목적 함수

#### GANs의 장점과 한계
- GANs의 결과물은 상대적으로 뚜렷하고, 빠르게 생성할 수 있다.
- 생성 모델이 판별 모델을 속일 수 있는 일부 데이터만 생성하는 현상이 생길 수 있다.
- 다양성이 낮다.

<br> 

#### 생성 모델 구현
- 풀링(Pooling) 레이어와 완전연결(Fully Connected) 레이어를 대체하여 전치 합성곱(Tranpose Convolution)을 사용하여 깊은 신경망을 구성한다.
- 전치 합성곱 레이어의 가중치를 평균 0, 표준편차 0.02인 정규분포로 초기화한다.
- ConvTranspose2d()를 사용해 입력의 크기를 키운다.
- 출력 전 레이어를 제외하고 배치 정규화(Batchnorm)을 사용한다.
- ReLU 활성화 함수를 사용한다. 마지막 출력을 위해서는 tanh 활성화 함수를 사용한다.


#### 판별 모델 구현
- 합성곱(Convolution) 레이어만 사용하여 깊은 신경망을 구성한다.
- 입력 데이터에 연결된 레이어를 제외하고 배치 정규화(Batchnorm)을 적용한다. # 생성자의 반대.
- LeakyReLU 활성화 함수를 사용한다. 마지막 레이어에서는 시그모이드 활성화 함수를 적용하여 [0,1] 범위의 입력 이미지가 실제 이미지일 확률을 출력한다.


<br>

#### 학습 파라미터: 0.0002, Adam B(베타)1: 0.5

#### 학습과정: 판별자와 생성자를 학습할 때 모두 생성자의 출력 이미지  G(z) 가 사용된다. pytorch에서는 판별자를 먼저 학습할 때 torch.detach()를 사용하여 생성자에게 역전파가 일어나지 않도록 그래디언트를 계산하지 않게 한다.
- fake_prob = discriminator(fake_data.detach()).view(-1) # D(G(z)) 계산하여 1차원 벡터로 변환, 판별자 학습 시, 반드시 detach() 붙여줘야한다.
- fake_prob = discriminator(fake_data).view(-1) # D(G(z)) 계산하여 1차원 벡터로 변환. 위의 fake_prob과 동일. 생성자 학습 시, detach()만 제거
- z를 정규분포에서 샘플링한다.
- 생성 이미지  G(z) 를 계산한다.
- 판별자의 손실 함수를 계산하고 판별자를 최적화한다.
- 생성자의 손실 함수를 게산하고 생성자를 최적화한다.
