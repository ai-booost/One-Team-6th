### 2025-03-06

### TIL

#### 전처리의 중요성
<img src="https://raw.githubusercontent.com/JeongJunSeong/One-Team-6th/jjs/TIL/jjs/Image/KakaoTalk_20250307_094037456.jpg" width="85%">
<br>


#### NLP 전처리
- html 태그, 특수문자, 이모티콘
- 정규표현식
- 불용어 (Stopword)
- 어간추출 (Stemming)
- 표제어추출 (Lemmatizing)
- 구두점
<img src="https://raw.githubusercontent.com/JeongJunSeong/One-Team-6th/jjs/TIL/jjs/Image/KakaoTalk_20250307_094037456_01.jpg" width="85%">
<br>

#### Tokenization
- 주어진 데이터를 토큰(Token) 단위로 나누는 작업
- 토큰이 되는 기준은 여러가지: 어절, 단어, 형태소, 음절, 자소 등)
- Character-based Tokenization: 한 글자(문자) 단위로 쪼개는 방식
- Word-based Tokenization: 공백이나 구두점 등을 기준으로 단어 단위로 쪼개는 방식
- Subword-based Tokenization: 단어를 더 작은 의미 있는 단위(Subword)로 분리하는 방식. ("토큰화를 진행합니다" → ['토큰', '##화', '##를', '진행', '##합니다']) 구현하는게 복잡하지만, WordPiece, BPE(Byte Pair Encoding), Unigram 같은 알고리즘이 있음.
<img src="https://raw.githubusercontent.com/JeongJunSeong/One-Team-6th/jjs/TIL/jjs/Image/스크린샷%202025-03-07%20091402.png" width="85%">
<br>


- Sentence Tokenizing: 마침표(.), 물음표(?), 느낌표(!) 등의 구두점을 기준으로 문장을 구분
- Word Tokenizing: 문장을 단어 단위로 쪼개는 과정. 일반적으로 공백이나 구두점을 기준으로 나누지만, 언어마다 다름 (한국어 형태소 분석)
<img src="https://raw.githubusercontent.com/JeongJunSeong/One-Team-6th/jjs/TIL/jjs/Image/스크린샷%202025-03-07%20091925.png" width="85%">
<br>

<br>

#### 토큰화 하는 이유: 단어 의미를 밀집 벡터로 단어들을 사전화
<img src="https://raw.githubusercontent.com/JeongJunSeong/One-Team-6th/jjs/TIL/jjs/Image/KakaoTalk_20250307_094037456_02.jpg" width="85%">
<br>

#### 토큰화 시 고려사항
- 구두점이나 특수 문자를 단순 제외
- 줄임말과 단어 내 띄어쓰기(하나의 단어에도 띄어쓰기를 하는게 있음)
- Sentence Tokenizing: 단순하게 마침표만으로 자를 수 없음
- 특히 한국어가 '조사'라는 것도 존재하고, 띄어쓰기를 안 지킴 => 형태소 단위의 토큰화가 필요함

<br>


#### 전처리에 쓰는 Tool
- KoNLPy: 한국어 자연언어처리 library
- NLTK: 영어 자연언어처리 library (한국어도 조금 할 수 있다)


#### KoNLPy
- morphs(): 형태소 추출
- pos(): 품사 태깅
- nouns(): 명사 추출
<img src="https://raw.githubusercontent.com/JeongJunSeong/One-Team-6th/jjs/TIL/jjs/Image/KakaoTalk_20250307_094037456_03.jpg" width="75%">
<br>


#### SentencePiece
- 구글이 공개한 Tokenization 도구
- BPE, unigram 같은 Subword-based Tokenization을 지원함

#### Hugging Face: 자연어처리에서 범용적으로 사용되는 대표적인 라이브러리

<br>

#### 텍스트 정제 (Cleaning)
- 코퍼스(텍스트 데이터의 집합) 내에서 토큰화 작업에 방해가 되거나 의미가 없는 텍스트, 노이즈를 제거하는 작업
- 불용어(stop word): 분석에 큰 의미가 없는 단어. 전처리 시, 불용어를 취급할 대상을 정의하는 작업이 필요. (NLTK에서는 여러 불용어를 사전에 정의해놨다.)
<img src="https://raw.githubusercontent.com/JeongJunSeong/One-Team-6th/jjs/TIL/jjs/Image/KakaoTalk_20250307_094037456_04.jpg" width="85%">
<br>



#### 정규화(Normalization)
- 실질적 의미를 뽑거나 단어의 원형을 뽑아주지만 좀 느리다는 단점이 있음.
- Stemming(어간 추출): 어형이 변형된 단어로부터 접사 등을 제거하고 어간을 분리. (포터 스태머 알고리즘)
<img src="https://raw.githubusercontent.com/JeongJunSeong/One-Team-6th/jjs/TIL/jjs/Image/KakaoTalk_20250307_094037456_05.jpg" width="85%">
<br>



- Lemmatization(표제어 추출): 품사 정보가 기본형으로 변환. 형태소 변환
<img src="https://raw.githubusercontent.com/JeongJunSeong/One-Team-6th/jjs/TIL/jjs/Image/KakaoTalk_20250307_094037456_06.jpg" width="65%">
<br>


#### 편집거리 (Edit distance)
- Levenshtein distance: 하나의 string s1을 s2로 변환하는 최소 횟수(두 string 간의 거리). 낮을수록 유사한 문자열로 판단
<img src="https://raw.githubusercontent.com/JeongJunSeong/One-Team-6th/jjs/TIL/jjs/Image/KakaoTalk_20250307_094625526.jpg" width="85%">
<br>



#### 정규표현식 (re)
- 특정한 규칙을 가진 문자열의 집합을 표현하는 데에 사용하는 형식 언어
- 원하는 규칙에 해당하는 문자만 남기거나, 제거 (이메일, 주민번호)


