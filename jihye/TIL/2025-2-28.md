## 한것
온라인 필수강의

복습
- 오토 인코더의 이해 ~ 벡터 양자화 변분 오토 인코더의 이해

수강
- 적대적 생성 신경망
- 조건부 생성 모델

## 배운것

#### 오토 인코더의 이해
입력 데이터의 패턴을 학습하여 데이터를 재건하는 모델  
- 인코더(Encoder): 데이터를 저차원 잠재 표현으로 요약
- 디코더(Decoder): 저차원 잠재 표현으로부터 데이터를 재구성(Reconstruction)

디노이징 오토 인코더
- 노이즈가 없는 원래 데이터로 재구성
- 데이터의 특성들을 더욱 정확히 학습함
- 미세하게 변형된 데이터도 같은 잠재 벡터로 표현

활용
- 특징 추출기로의 활용
- 분류, 클러스터링 문제 해결
- 이상치 탐지

#### 변분 오토 인코더의 이해
변분 오토 인코더(VAE)
- 오토 인코더와 동일한 구조(Encoder + Decoder)를 가지는 생성 모델
- 잠재 벡터의 분포: 표준정규분포
-  z벡터를 학습하는 게 아니라,
각 데이터에 대해 정규 분포를 학습해서 그 분포에서 샘플링한 값을 z 로 사용
- VAE의 인코더(Encoder) 는 입력  x  를 단일  z값이 아니라 확률 분포  q(z|x)  로 변환한다.
- z들이 서로 가까이 모여서 연속적인 공간을 형성


VAE의 목표
- q(z|x)가  p(z|x)에 가깝도록 학습
→ KL Divergence 를 사용해 q(z|x) 와  p(z)간의 차이를 줄여준다.
- p(x|z) :  z가 주어졌을 때, 원래 데이터를 복원할 확률 → 디코더(Decoder)
- q(z|x) : 입력 x가 주어졌을 때, z를 추론할 확률 → 인코더(Encoder)

VAE의 손실 함수  
Evidence Lower Bound (ELBO) 를 최대화하는 방식으로 학습된다.
ELBO는 크게 두 가지 손실 항으로 나눌 수 있다.

① 재구성 손실 (Reconstruction Loss)
- 디코더가 샘플링한  z  를 사용해서 원래 입력  x  를 잘 복원하도록 학습한다.
- 보통 MSE(Mean Squared Error) 또는 Cross-Entropy 손실을 사용한다.
	
→ 즉,  z  를 샘플링한 후, 원래 데이터  x  를 복원하는 확률을 최대화!

② 정규화 손실 (KL Divergence)  
- 잠재 변수 z가 N(0,1)을 따르도록 유도하는 항목이다.
- VAE에서는 q(z|x)가 특정한 분포를 따르게 한다.
- KL Divergence를 사용해서 이 두 분포가 비슷해지도록 만든다.

→ 즉, 학습된 평균과 표준편차가 표준 정규분포에 가까워지도록 강제!

재매개변수화 트릭 (Reparameterization Trick)

- 학습 과정에서 샘플링을 하면 미분이 불가능해지는 문제를 해결하기 위해 사용

#### 벡터 양자화 변분 오토 인코더의 이해

VQVAE
- 이미지나 텍스트는 이산표현으로 표현알 수 있다.
- 이산 잠재변수를 갖음
- 임베딩 벡터도 학습, 데이터 특성을 표현하는 단어를 학습하고 수정할 수 있게 한다

벡터 양자화(Vector Quantization, VQ)
- 인코더가 만든 연속적인 잠재 벡터를 가까운 코드북(codebook)의 벡터로 강제 매핑
- 코드워드 - 이미지를 나타내는 특성

ELBO
- 복원오차: 입력과 출력이 동일해야함

PixelCNN
- 코드워드의 분포를 학습

VQVAE-2

고해상도 이미지를 다루기 위해 계층 구조 사용
- 레벨별로 생성, 조건을 달아줌
- 위층(Top level): 전역적인(Global) 특징
- 아래층(Bottom level): 국소적인(Local) 특징

사전 분포 학습
- 위층: Self-attention 사용 (PixelSNAIL) → 전역적인 정보를 활용
- 아래층: 위층의 잠재 벡터를 추가로 입력으로 받음

생성된 데이터 기각 샘플링(Rejection Sampling)

- 사전 학습된 분류기를 활용
- 생성된 데이터가 실제 데이터와 유사하다면 정답 레이블 예측 확률이 높을 것!
- 예측 확률이 낮은 것은 제외

#### 적대적 생성 신경망

생성된 데이터와 실제 데이터를 판별하고 속이는 과정을 거치며 생성 모델을 개선  
데이터를 생성하는 생성 모델 (Generator)과 데이터의 진위를 구별하는 판별 모델 (Discriminator)로 구성

임이의 노이즈를 입력으로 받아 사용
-> 판별기로 생성모델을 개선

GANs 목적 함수
- GANs의 목적 함수는 생성 데이터 분포와 실제 데이터 분포가 동일한 pg = pdata에서 최적

GANs 학습 방식

- 두 모델이 서로 적대적인 방향으로 훈련
- 적대적 학습: 생성 모델은 판별 모델의 출력값을 최소화; 판별 모델은 출력값을 최대화
- 손실 함수의 최소화 문제: 기울기 하강 ⇔ 최대화 문제: 기울기 상승

- 판별 모델의 기울기를 조정하면 학습이 더 잘 될 것 = 평평한 기울기를 가파르게  -> 생성자 기준에서 판별 모델이 정답을 맞출 가능성을 최소화하는 것 대신, 틀릴 가능성을 최대화

VAE와 GANs의 생성 결과 비교
- VAE의 - 상대적으로 흐릿하고, 입력 데이터와 유사한 형태로 생성
- GANs의 - 상대적으로 뚜렷하고, 입력 데이터와 다른 형태의 데이터를 생성

#### 조건부 생성 모델

다양한 활용을 위해 생성 데이터의 의미 제어 방법이 필요함
-> 임의의 잠재 벡터 + 조건 정보를 추가하여 데이터를 생성
- 생성 모델에 입력되는 잠재 벡터와, 판별 모델에 입력되는 조건부 벡터가 추가된 형태
범주(카테고리)부터 영상의 전체 구조(레이아웃)에 이르기까지 다양한 입력을 조건으로 받음

다양한 모드 기반의 조건부 생성
- 범주를 표현하는 벡터뿐 아니라, 텍스트, 이미지, 오디오 등으로부터 다양한 조건부 생성이 가능

- Pix2pix (2017): 이미지 대 이미지 변환
- LostGANs (2019) : 레이아웃 대 이미지 변환
- Speech2image (2020): 오디오 대 이미지 변환
- GigaGAN (2023): 텍스트 대 이미지 변환

## 느낀점

수학만 나오면 어렵다  
여러번 들어도 어렵다  
하나씩 천천히 이해하자