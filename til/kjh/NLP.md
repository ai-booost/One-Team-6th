# FASTCAMPUS
## NLP(자연어 처리)

### 개요
#### 1. 개념
컴퓨터가 자연언어의 의미를 분석하고 이해하여 생성할 수 있도록 만들어주는 기술  
- **자연어**: 사람들이 일상생활에서 자연스럽게 사용하는 언어(인공언어의 반대말)

#### 2. NLG vs NLU
- **NLG (Natural Language Generation)**: 자연어 생성, 데이터를 기반으로 의미 있는 문장을 생성하는 기술
- **NLU (Natural Language Understanding)**: 자연어 이해, 입력된 문장을 분석하여 의미를 이해하는 기술

### 자연어 처리가 어려운 이유
1. **중의적 표현**  
   - 언어는 최소한의 표현으로 최대한의 정보를 전달함.  
   - 문장에서 일반적인 정보의 생략이 빈번하여 의미론적인 중의성이 발생함.  
   - 예시:  
     - "길이 있다" → "살아날 길이 있다" / "지나갈 길이 있다"

2. **고유 명사의 처리**  
   - 이름, 회사명 등 특정한 의미를 가지는 단어를 정확히 인식하기 어려움.

3. **사전 미등록어 처리**  
   - 신조어, 줄임말 등이 계속 생성되므로 사전에 등록되지 않은 단어를 처리해야 함.

4. **문맥에 따른 모호성**  
   - 단어나 구문은 주변 문맥에 의해 의미가 변화함.  
   - 예시:  
     - "나는 밥을 먹었다" → 밥이 주식인 경우 '밥'은 '식사'를 의미하지만, 다른 문맥에서는 '밥'이 구체적인 음식을 의미할 수 있음.

### 왜 한국어에서 자연어 처리가 더 어려운가
1. **교착어 특성**  
   - 한국어는 어근과 접사에 의해 단어의 의미와 기능이 정해짐.  
   - 예: '그녀' 뒤에는 '가', '를', '의' 등의 다양한 접사가 붙을 수 있어 조합이 많음.

2. **단어 순서 및 주어 생략**  
   - 단어의 순서가 문장의 의미를 결정하는 요소가 아님.  
   - 주어가 자주 생략됨.

3. **띄어쓰기의 불완전성**  
   - 띄어쓰기가 정착되지 않아 분석이 어려움.  
   - 예: "지금 **막 차**가 떠났다" vs. "지금 **막차**가 떠났다"

4. **평서문과 의문문의 구조가 유사함**  
   - 문장 부호가 없을 경우 구분이 어려움.  
   - 많은 경우 문맥에 의존해야 함.

### 일상 속 자연어 처리 활용 사례
1. **문법 교정 (Grammatical Error Correction, GEC)**  
2. **음성 인식 (Speech Recognition)**  
3. **기계 번역 (Machine Translation)**  
4. **정보 추출 - 검색 (Information Extraction)**  
5. **질의 응답 (Question Answering)**  
6. **문서 요약 (Text Summarization)**  
7. **AI Chat Bot**  
8. **AI 기반 창작 (AI x Creation)**  
9. **자동완성 (Autocomplete)**  

---

## 언어학 기초
### 언어학
- 인간의 언어를 **과학적으로** 연구하는 학문  
- 전산언어학: 컴퓨터를 이용하여 언어를 자동 분석하고 처리하는 연구 분야

### 언어학의 접근 방법
1. **규칙기반 접근**: 문법 규칙을 활용한 방식  
2. **통계기반 접근**: 대량의 언어 데이터를 분석하여 확률적 규칙을 적용  
3. **딥러닝 기반 접근**: 신경망을 활용하여 자동적으로 언어 구조를 학습  

### 음절, 형태소, 어절, 품사
1. **음절**: 가장 작은 말소리 단위  
2. **형태소**: 의미를 가지는 최소 단위  
3. **어절**: 형태소가 모여 띄어쓰기로 구분된 단위  
4. **품사**: 단어를 문법적 의미에 따라 분류한 것  

---

### 언어학의 하위 분야
#### 1. 형태 (Form)
- **음운론 (Phonology)**: 말소리 연구  
- **형태론 (Morphology)**: 형태소와 단어 연구  
- **통사론 (Syntax)**: 문장 구조 연구  

#### 2. 내용 (Meaning)
- **의미론 (Semantics)**: 단어와 문장의 의미 연구  
- **구조적 모호성 (Structural Ambiguity)**  
  - 예: "나는 그녀를 좋아하는 친구와 이야기했다."  
    - ① 내가 좋아하는 친구와 이야기했다.  
    - ② 그녀를 좋아하는 친구와 이야기했다.  
- **의미 자질 (Semantic Features)**  
  - 단어의 의미를 자질들의 조합으로 표현  
  - 예:  
    - "남자" → [+인간, +성인, +남성]  
    - "여자" → [+인간, +성인, -남성]  

#### 3. 사용 (Pragmatics)
- **화용론 (Pragmatics)**: 상황에 따라 달라지는 언어의 의미 연구  
- **문맥(Context)**
  - **물리적 문맥**: 단어가 나타나는 물리적 환경  
  - **언어적 문맥**: 주변 단어와의 관계  
- **직시 표현 (Deixis)**
  - 문맥이 있어야 해석 가능한 표현 (ex: 이, 저, 그)  
- **화행 (Speech Act)**
  - 말하는 행위 자체가 특정한 의미를 가짐 (ex: "우체국이 어디인가요?" → 길을 묻는 행위)  

### 언어학의 적용 사례
1. **키워드 분석**  
   - 텍스트 내에서 중요한 키워드를 추출하여 주제나 핵심 정보를 파악
2. **토큰화 (Tokenization)**  
   - 문장을 단어, 음절, 형태소 등으로 나누어 분석 가능한 단위로 분리
3. **품사 태깅 (POS Tagging)**  
   - 단어를 품사에 따라 분류하여 문장 구조 파악
4. **구문 분석 (Parsing)**  
   - 문장의 구조를 분석하여 구문 트리 형태로 표현
5. **의미/담화 분석**  
   - 문장과 담화의 의미를 분석하여 더 깊은 이해를 도모
6. **개체명 인식 (NER, Named Entity Recognition)**  
   - 텍스트에서 사람, 장소, 기관 등의 고유명사를 식별
7. **문법 교정 (GEC)**  
   - 텍스트에서 문법적인 오류를 찾아 수정하는 기술
8. **의존 구문 분석 (Dependency Parsing)**  
   - 문장의 구성 요소들이 서로 어떻게 의존하는지 분석
9. **BERT**  
   - 다양한 층에서 언어 정보를 반영  
     - 아래층: 표면적 특징  
     - 중간층: 구문적 특징  
     - 상층: 의미론적 특징
10. **LIMIT-BERT (Linguistic Informed Multi-Task BERT)**  
   - 언어적 지식을 결합하여 다양한 작업을 동시에 처리하는 모델
11. **GiBERT**  
   - 특정 언어의 구조적 특성을 반영한 BERT 모델
12. **DMOps (Data Management Operation and Recipes)**  
   - 텍스트 분석에서 데이터 관리와 최적화 작업을 지원하는 프레임워크
13. **텍스트 요약 (Text Summarization)**  
   - 긴 텍스트를 압축하여 핵심적인 정보를 요약하는 기술  
     - 추출적 요약: 중요한 문장이나 구를 그대로 추출  
     - 생성적 요약: 새로운 문장을 생성하여 요약
14. **감정 분석 (Sentiment Analysis)**  
   - 텍스트에서 감정적 상태나 의견을 추출하여 긍정, 부정, 중립으로 분류  
15. **질의 응답 시스템 (Question Answering)**  
   - 주어진 질문에 대해 적절한 답을 텍스트에서 찾아 제공하는 시스템

## 텍스트 전처리
### 개요
#### 전처리(Preprocessing) 
1. HTML 태그, 특수문자, 이모티콘
2. 정규표현식
3. 불용어(Stopword)
4. 어간 추출(Stemming)
5. 표제어추출(Lemmatizing)

**preprocessing pipeline**  
문서 → 토큰화(정규표현식/트위터) → 텍스트 전처리(불용어 제거, 어근추출, 구두점(punctuation)) → 단어주머니

##### Preprocessing Tools
**KoNLPy**
- 한국어 자연언어처리를 위한 대표적인 Python Library
- Twitter, Komoran, Mecab 등 다양한 형태소 분석기들을 제공

**NLTK**
- 영어로 된 텍스트의 자연처리를 위한 대표적인 Python Library
- Classification, Tokenization 등 50개가 넘는 library를 제공하며 쉬운 interface 제공

#### 토큰화(Tokenization)
주어진 데이터를 토큰이라 불리는 단위로 나누는 작업  
토큰이 되는 기준은 다를 수 있음  
- character-based Tokenization  
- word-based Tokenization  
- Subword-based Tokenization

1. 문장 토큰화: 문장 분리  
2. 단어 토큰화: 구두점 분리, 단어 분리

토큰화가 중요한 이유: 단어 의미를 밀집 벡터로 표현하기 위해 단어들을 사전화

**고려사항**
1. 구두점이나 특수 문자를 단순히 제외
2. 줄임말과 단어 내 띄어쓰기
3. 문장 토큰화: 단순 마침표를 기준으로 자를 수 없음

**한국어 토큰화의 어려움**  
영어는 합성어나 줄임말에 대한 예외처리만 한다면 띄어쓰기를 기준으로 토큰화가 잘 작동하지만, 한국어는 조사 등의 존재로 띄어쓰기가 불완전해 형태소 단위의 토큰화가 필요합니다.

##### Tokenization Tools
**KoNLPy**
- morphs: 형태소 추출
- pos: 품사 태깅
- nouns: 명사 추출

**SentencePiece**
- Google이 공개한 Tokenization 도구
- BPE, unigram 방식 등 다양한 subword units를 지원

**Tokenizers**
- Huggingface의 대표적인 자연어처리 라이브러리
- 다양한 언어모델들의 Tokenization과 어휘사전 등을 지원

#### 정제(Cleansing)
코퍼스 내에서 의미가 없는 텍스트나 노이즈를 제거하는 작업
- 주로 불용어, 특수문자 제거, 대소문자 통합, 중복 문구 제거, 다중공백 통일 등

1. 불용어
   - 분석에 의미가 없는 단어로 코퍼스 내에서 자주 등장하나 실질적인 의미가 없는 단어들

**NLTK에서는 불용어 사전이 이미 정의되어 있음.**

#### 정규화(Normalization)
- **Stemming (어간 추출)**: 단어에서 접사를 제거하고 어간을 추출
- **Lemmatization (표제어 추출)**: 품사 정보를 보존하며 기본형으로 변환

#### 편집 방법 (Edit Operations)
Levenshtein Distance: 두 문자열 간의 **최소 변환 횟수**를 계산하는 방법  

1. **삭제(Delete)**  
   - 문자열에서 하나의 문자를 제거하는 연산.  
   - 예: "kitten"에서 "k"를 삭제하면 "itten"이 됨.

2. **삽입(Insert)**  
   - 문자열에 하나의 문자를 삽입하는 연산.  
   - 예: "kitten"에 "s"를 삽입하면 "kittens"이 됨.

3. **대체(Substitution)**  
   - 문자열에서 하나의 문자를 다른 문자로 바꾸는 연산.  
   - 예: "kitten"에서 "k"를 "s"로 대체하면 "sitten"이 됨.

이 세 가지 연산은 서로 결합되어 최단 경로로 문자열을 변환하는 데 사용됨. 편집 거리 알고리즘은 주어진 두 문자열을 비교하면서 최소한의 연산으로 한 문자열을 다른 문자열로 변환할 수 있는 방법을 찾음.

##### 예시

- 문자열 "kitten"과 "sitting"의 편집 거리를 구하는 과정:
  1. "k"를 "s"로 대체 → "sitten"
  2. "e"를 "i"로 대체 → "sittin"
  3. "n"을 삽입 → "sitting"

위의 과정에서는 총 3번의 연산이 필요하므로 편집 거리는 3.


#### 정규표현식
특정한 규칙을 가진 문자열을 표현하는 언어로, 복잡한 문자열 검색과 치환에 사용되며, Python에서는 `re` 라이브러리로 처리 가능  
- 복잡한 패턴을 빠르게 처리할 수 있음
